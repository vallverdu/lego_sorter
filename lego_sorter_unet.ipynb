{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzK__LLLzRPG"
      },
      "source": [
        "Install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGOV9TZ8zYzH"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install Pillow\n",
        "!pip install imgaug\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Load TensorBoard Extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqzx9MHdzXj8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTSm4pA10SLj"
      },
      "source": [
        "download the dataset from drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZpzr5a50VRf"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/data_lego.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcKfRPU93RUj"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgjNxB9E1y0z"
      },
      "source": [
        "code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTiv3XA014l6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import imgaug.augmenters as iaa\n",
        "import imgaug as ia\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    OUTPUT = \"/content/drive/MyDrive\"\n",
        "    DATASET_PATH = \"/content/data\"\n",
        "    LABELS_PATH = os.path.join(DATASET_PATH, \"data.csv\")\n",
        "    IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
        "    SUMMARY_PATH = os.path.join(DATASET_PATH, \"summary\")\n",
        "    DEBUG_PATH = os.path.join(DATASET_PATH, \"examples\")\n",
        "    IMAGE_RESIZE = 256\n",
        "    AUGMENTATION_FACTOR = 30\n",
        "    TRAIN_SPLIT= 0.8\n",
        "    BATCH_SIZE = 32\n",
        "    FREEZE_BACKBONE = True\n",
        "    EPOCHS = 60\n",
        "    DEBUG = True\n",
        "    MODEL_NAME = \"ckpt_{epoch:d}_{loss:f}.pth\"\n",
        "    CKPT_SAVE_INTERVAL = 5\n",
        "    LR = 1e-4\n",
        "    EPS = 1e-6\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lego dataset definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LegoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, rgb_transform=None,mask_transform=None, do_aug=False, augmentation_factor=1):\n",
        "        self.data = pd.read_csv(os.path.join(root_dir,csv_file))\n",
        "        self.root_dir = root_dir\n",
        "        self.rgb_transform = rgb_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.do_aug = do_aug\n",
        "        self.brick_type_dict = {\n",
        "                        '6*2': 0,\n",
        "                        '3*1': 1,\n",
        "                        '3*2': 2,\n",
        "                        '2*1': 3,\n",
        "                        '4*2': 4,\n",
        "                        '1*1': 5,\n",
        "                        '8*2': 6,\n",
        "                        '4*1': 7,\n",
        "                        '2*1_pyramid': 8,\n",
        "                        '2*2': 9,\n",
        "                        '6*1': 10\n",
        "                    }\n",
        "        \n",
        "        self.seq_train = iaa.Sequential([\n",
        "            iaa.CropAndPad(percent=(0, 0.1)),\n",
        "            iaa.Affine(rotate=(-20, 20)),\n",
        "            iaa.Resize({\"height\": Config.IMAGE_RESIZE, \"width\": Config.IMAGE_RESIZE}),\n",
        "            iaa.Affine(shear=(-15, 15)),\n",
        "            iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}),\n",
        "            iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}),\n",
        "            iaa.Flipud(0.3),\n",
        "            iaa.Fliplr(0.3)\n",
        "        ])\n",
        "\n",
        "        self.seq_color = iaa.Sequential([\n",
        "            iaa.MultiplyAndAddToBrightness(mul=(0.5, 1.5), add=(-30, 30)),\n",
        "            iaa.GammaContrast((0.5, 2.0), per_channel=True),\n",
        "            iaa.GaussianBlur((0, 3.0)),\n",
        "            iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
        "            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
        "            iaa.LinearContrast((0.75, 1.5)),\n",
        "            iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
        "            iaa.AddToHueAndSaturation((-20, 20))\n",
        "        ])\n",
        "        \n",
        "    \n",
        "\n",
        "        self.seq_test = iaa.Sequential([\n",
        "            iaa.Resize({\"height\": Config.IMAGE_RESIZE, \"width\": Config.IMAGE_RESIZE})\n",
        "        ])\n",
        "\n",
        "        self.augmentation_factor = augmentation_factor\n",
        "        self.do_aug = do_aug\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) * self.augmentation_factor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Determine the original index in the annotations\n",
        "        original_index = idx // self.augmentation_factor\n",
        "        sample = self.data.iloc[original_index]\n",
        "\n",
        "        brick_name = sample['filename']\n",
        "        img_path = os.path.join(self.root_dir,'images', f\"{brick_name}_rgb.png\")\n",
        "        image = np.array(Image.open(img_path))[..., :3]\n",
        "\n",
        "        # Load the binary mask for each image\n",
        "        mask_path = os.path.join(self.root_dir, 'images', f\"{brick_name}_mask.png\")\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
        "        mask = mask[..., np.newaxis]  # Add an extra channel dimension at the end\n",
        "        mask = mask / 255.  # Normalize to [0, 1]\n",
        "\n",
        "        \n",
        "\n",
        "        # Extract target variables\n",
        "        brick_type = self.brick_type_dict[sample['brick_type']]\n",
        "\n",
        "       \n",
        "        # Apply augmentations\n",
        "        if self.do_aug:\n",
        "            # Augment both image and mask\n",
        "            augmentations = self.seq_train.to_deterministic()  # Ensures both image and mask undergo the same augmentations\n",
        "            image = augmentations.augment_image(image)\n",
        "            mask = (augmentations.augment_image(mask) > 0.5).astype(np.uint8) #  Ensures that after the spatial augmentations, mask remains strictly binary\n",
        "\n",
        "            # Apply color-related augmentations only to the image\n",
        "            image = self.seq_color.augment_image(image)\n",
        "        else :\n",
        "            image = self.seq_test(image=image)\n",
        "            mask = self.seq_test(image=mask)\n",
        "        \n",
        "        image = Image.fromarray(image)\n",
        "        mask = Image.fromarray((mask.squeeze() * 255).astype(np.uint8))\n",
        "\n",
        "        if self.rgb_transform:\n",
        "            image = self.rgb_transform(image)\n",
        "        \n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "        \n",
        "        return (\n",
        "            image,\n",
        "            mask,\n",
        "            torch.tensor(brick_type).long()\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Early stop system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, delta=0, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## UNet architecture Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LegoModelUNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes = 11, inference = True):\n",
        "        super(LegoModelUNet, self).__init__()\n",
        "        \n",
        "        self.inference = inference\n",
        "\n",
        "        # Contracting Path\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "        \n",
        "        # Expansive Path\n",
        "        self.upconv4 = self.upconv_block(1024, 512)\n",
        "        self.upconv3 = self.upconv_block(512, 256)\n",
        "        self.upconv2 = self.upconv_block(256, 128)\n",
        "        self.upconv1 = self.upconv_block(128, 64)\n",
        "        \n",
        "        # Segmentation head\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            nn.Conv2d(64, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        \n",
        "        # Classification head\n",
        "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        \n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    \n",
        "    def upconv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Contracting Path\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(F.max_pool2d(e1, 2))\n",
        "        e3 = self.enc3(F.max_pool2d(e2, 2))\n",
        "        e4 = self.enc4(F.max_pool2d(e3, 2))\n",
        "        \n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(F.max_pool2d(e4, 2))\n",
        "        \n",
        "        # Expansive Path\n",
        "        d4 = self.upconv4(b)\n",
        "        d3 = self.upconv3(d4 + e4)\n",
        "        d2 = self.upconv2(d3 + e3)\n",
        "        d1 = self.upconv1(d2 + e2)\n",
        "        \n",
        "        # Segmentation head\n",
        "        seg_output = self.segmentation_head(d1 + e1)\n",
        "        \n",
        "        # Classification head\n",
        "        avg_pool = self.global_avg_pooling(b).view(b.size(0), -1)\n",
        "        fc1 = F.relu(self.fc1(avg_pool))\n",
        "        class_output = F.softmax(self.fc2(fc1), dim=1)\n",
        "        \n",
        "        return seg_output, class_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualizeDataset(csv_file,root_dir,rgb_transform,mask_transform,do_aug,augmentation_factor) :\n",
        "    # Load the dataset\n",
        "    dataset = LegoDataset('data.csv', './data', rgb_transform=rgb_transform, mask_transform = mask_transform, do_aug=do_aug, augmentation_factor=augmentation_factor)\n",
        "\n",
        "    # Define the inverse normalization function\n",
        "    inv_normalize = transforms.Normalize(\n",
        "                        mean = [ -0.485/0.229, -0.456/0.224, -0.406/0.225 ],\n",
        "                        std = [ 1/0.229, 1/0.224, 1/0.225 ])\n",
        "    \n",
        "      \n",
        "    # Randomly select some samples\n",
        "    indices = np.random.choice(len(dataset), 5)\n",
        "\n",
        "    for index in indices:\n",
        "\n",
        "        image, mask, _ = dataset[index]\n",
        "\n",
        "        # Convert tensors to numpy arrays for visualization\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            image = inv_normalize(image)  # Apply the inverse normalization\n",
        "            image = image.permute(1, 2, 0).numpy()\n",
        "        if isinstance(mask, torch.Tensor):\n",
        "            mask = mask.squeeze().numpy()\n",
        "\n",
        "        # Check the min and max of the image\n",
        "        print(f\"Image min: {image.min()}, max: {image.max()}\")\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "        axes[0].imshow(image)\n",
        "        axes[0].set_title(\"RGB Image\")\n",
        "        axes[1].imshow(mask, cmap='gray')\n",
        "        axes[1].set_title(\"Mask\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, device, epoch):\n",
        "\n",
        "    data_loader.dataset.dataset.do_aug = True\n",
        "    data_loader.dataset.dataset.augmentation_factor = Config.AUGMENTATION_FACTOR\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_iou = 0.0\n",
        "    \n",
        "    pbar = tqdm(data_loader)\n",
        "\n",
        "    for batch_idx, (\n",
        "        image,\n",
        "        mask,\n",
        "        brick_type\n",
        "    ) in enumerate(pbar, 0):\n",
        "        \n",
        "        image = image.to(device)\n",
        "        mask = mask.to(device)\n",
        "        brick_type = brick_type.to(device)\n",
        "        \n",
        "        # Optimizer zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Model inference\n",
        "        out_mask, out_brick_type = model(image)\n",
        "        \n",
        "        \n",
        "        if len(mask.shape) == 5:\n",
        "            mask = mask.squeeze(2).squeeze(3)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss_brick = criterion_brick_type(out_brick_type, brick_type)\n",
        "        loss_mask = F.binary_cross_entropy(out_mask, mask)   # BCE loss for segmentation\n",
        "        \n",
        "        # Combine the losses\n",
        "        loss = loss_brick + loss_mask\n",
        "\n",
        "        # Compute IoU\n",
        "        iou = compute_iou(out_mask, mask)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_iou += iou\n",
        "        \n",
        "    avg_loss = train_loss / len(data_loader)\n",
        "    avg_iou = train_iou / len(data_loader)\n",
        "\n",
        "    return avg_loss, avg_iou\n",
        "\n",
        "def test(model, data_loader, device, epoch):\n",
        "\n",
        "    data_loader.dataset.dataset.do_aug = False\n",
        "    data_loader.dataset.dataset.augmentation_factor = 1\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss_brick_type = 0.0\n",
        "    test_loss = 0.0\n",
        "    test_iou = 0.0\n",
        "    \n",
        "    # Initialize accumulators\n",
        "    total_type_correct = 0\n",
        "   \n",
        "\n",
        "    pbar = tqdm(data_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_idx, (\n",
        "            image,\n",
        "            mask,\n",
        "            brick_type\n",
        "        ) in enumerate(pbar, 0):\n",
        "            \n",
        "            image = image.to(device)\n",
        "            mask = mask.to(device)\n",
        "            brick_type = brick_type.to(device)\n",
        "       \n",
        "            # Forward pass\n",
        "            out_mask, out_brick_type = model(image)\n",
        "\n",
        "           # Calculate loss\n",
        "            loss_brick = criterion_brick_type(out_brick_type, brick_type)\n",
        "            loss_mask = F.binary_cross_entropy(out_mask, mask)   # BCE loss for segmentation\n",
        "            \n",
        "            # Combine the losses\n",
        "            loss = loss_brick + loss_mask\n",
        "\n",
        "            # Compute IoU\n",
        "            iou = compute_iou(out_mask, mask)\n",
        "\n",
        "            pbar.set_description(f\"TEST loss={float(loss)}\")\n",
        "\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "            test_iou += iou\n",
        "            \n",
        "            # Brick accuracy\n",
        "            type_correct = (torch.argmax(out_brick_type, dim=1) == brick_type).float().sum()\n",
        "            total_type_correct += type_correct\n",
        "\n",
        "     \n",
        "    avg_loss = test_loss / len(data_loader)\n",
        "    avg_iou = test_iou / len(data_loader)\n",
        "\n",
        "    # return avg_loss, loss_brick, loss_rotation, loss_color, type_correct\n",
        "    return avg_loss, avg_iou, type_correct\n",
        "\n",
        "def criterion_values(pred, true):\n",
        "    return F.mse_loss(torch.sigmoid(pred), true)\n",
        "  \n",
        "criterion_brick_type = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def compute_iou(pred_mask, true_mask):\n",
        "    # True Positive: Predicted = 1, Ground Truth = 1\n",
        "    intersection = (pred_mask * true_mask).float().sum()\n",
        "    \n",
        "    # True Positive + False Positive + False Negative\n",
        "    union = pred_mask.float().sum() + true_mask.float().sum() - intersection\n",
        "\n",
        "    iou = intersection / union\n",
        "    return iou.item()\n",
        "\n",
        "# Define the data transformations\n",
        "rgb_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8OBB4Ahxdu5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main(experiment):\n",
        "    \n",
        "    torch.manual_seed(108)\n",
        "    np.random.seed(108)\n",
        "\n",
        "    writer = SummaryWriter(f\"runs/{experiment}\")\n",
        "\n",
        "    # define Device\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "    # Create project folder and names\n",
        "    OUTPUT_FOLDER = os.path.join(Config.OUTPUT, experiment, datetime.datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\"))\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    print('OUTPUT_FOLDER',OUTPUT_FOLDER)\n",
        "\n",
        "\n",
        "\n",
        "    os.makedirs(Config.SUMMARY_PATH, exist_ok=True)\n",
        "    os.makedirs(Config.DEBUG_PATH, exist_ok=True)\n",
        "\n",
        "    CHECKPOINTS_FOLDER = os.path.join(OUTPUT_FOLDER, 'checkpoints')\n",
        "    os.makedirs(CHECKPOINTS_FOLDER, exist_ok=True)\n",
        "\n",
        "    # Define the data transformations\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Define the dataset and data loader\n",
        "    lego_dataset = LegoDataset('data.csv', './data', rgb_transform=rgb_transform, mask_transform = mask_transform, do_aug=True, augmentation_factor=Config.AUGMENTATION_FACTOR)\n",
        "    train_size = int(Config.TRAIN_SPLIT * len(lego_dataset))\n",
        "    test_size = len(lego_dataset) - train_size\n",
        "    print('train_size',train_size)\n",
        "    print('test_size',test_size)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(lego_dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialize the model and define the loss function and optimizer\n",
        "    model = LegoModelUNet(in_channels=3, inference=False)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.LR, eps = Config.EPS)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Epochs\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    best_model = None\n",
        "    best_type_accuracy = None\n",
        "\n",
        "    # Initialize accumulators\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    epochs_type_accuracy  = []\n",
        "\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(Config.EPOCHS):\n",
        "        \n",
        "        \n",
        "        train_loss, train_iou = train(model, train_loader, optimizer, device, epoch)\n",
        "        val_loss, val_iou, total_type_correct = test(model, test_loader, device, epoch)\n",
        "\n",
        "        writer.add_scalar('Training Loss', train_loss, epoch)\n",
        "        writer.add_scalar('Validation Loss', val_loss, epoch)\n",
        "        writer.add_scalar('Training IoU', train_iou, epoch)\n",
        "        writer.add_scalar('Validation IoU', val_iou, epoch)\n",
        "        writer.add_scalar('Validation Accuracy', total_type_correct, epoch)\n",
        "\n",
        "\n",
        "        \n",
        "        # Compute average metrics for the epoch\n",
        "        epoch_type_accuracy = total_type_correct / len(lego_dataset) * 100\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            best_model = model.state_dict()\n",
        "            best_gender_accuracy = epoch_type_accuracy\n",
        "        \n",
        "        # Checkpointing\n",
        "        if epoch > 0 and epoch % Config.CKPT_SAVE_INTERVAL == 0 :\n",
        "            print(f\"Saved checkpoint {epoch}\\n\")\n",
        "            model_path_solve = os.path.join(CHECKPOINTS_FOLDER, f\"ckpt_{epoch}_{val_loss}.pth\")\n",
        "            optimizer_path_solve = os.path.join(CHECKPOINTS_FOLDER, model_path_solve.replace('ckpt_', 'optim_'))\n",
        "            torch.save(model.state_dict(), model_path_solve)\n",
        "            torch.save(optimizer.state_dict(), optimizer_path_solve)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch + 1}/{Config.EPOCHS}\\n\"\n",
        "          f\"Type accuracy: {epoch_type_accuracy:.2f}%\\n\"\n",
        "          f\"Validation loss: {val_loss:.4f}\")\n",
        "        \n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Check for early stopping\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "       \n",
        "\n",
        "    # Save the best model\n",
        "    torch.save(best_model, os.path.join(CHECKPOINTS_FOLDER, f\"best_{best_epoch}_{best_val_loss}.pth\"))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QwG_zQqk35Y-"
      },
      "outputs": [],
      "source": [
        "main('model_lego_sorter_unet')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
