{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzK__LLLzRPG"
      },
      "source": [
        "Install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGOV9TZ8zYzH"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install Pillow\n",
        "!pip install imgaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqzx9MHdzXj8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTSm4pA10SLj"
      },
      "source": [
        "download the dataset from drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZpzr5a50VRf"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/data_lego.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcKfRPU93RUj"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgjNxB9E1y0z"
      },
      "source": [
        "code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTiv3XA014l6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import imgaug.augmenters as iaa\n",
        "import imgaug as ia\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    OUTPUT = \"/content/drive/MyDrive\"\n",
        "    DATASET_PATH = \"/content/data\"\n",
        "    LABELS_PATH = os.path.join(DATASET_PATH, \"data.csv\")\n",
        "    IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
        "    SUMMARY_PATH = os.path.join(DATASET_PATH, \"summary\")\n",
        "    DEBUG_PATH = os.path.join(DATASET_PATH, \"examples\")\n",
        "    IMAGE_RESIZE = 256\n",
        "    AUGMENTATION_FACTOR = 30\n",
        "    TRAIN_SPLIT= 0.8\n",
        "    BATCH_SIZE = 64\n",
        "    FREEZE_BACKBONE = True\n",
        "    EPOCHS = 60\n",
        "    DEBUG = True\n",
        "    MODEL_NAME = \"ckpt_{epoch:d}_{loss:f}.pth\"\n",
        "    CKPT_SAVE_INTERVAL = 5\n",
        "    LR = 1e-4\n",
        "    EPS = 1e-6\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lego dataset definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LegoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None, do_aug=False, augmentation_factor=1):\n",
        "        self.data = pd.read_csv(os.path.join(root_dir,csv_file))\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.do_aug = do_aug\n",
        "        self.brick_type_dict = {\n",
        "                        '6*2': 0,\n",
        "                        '3*1': 1,\n",
        "                        '3*2': 2,\n",
        "                        '2*1': 3,\n",
        "                        '4*2': 4,\n",
        "                        '1*1': 5,\n",
        "                        '8*2': 6,\n",
        "                        '4*1': 7,\n",
        "                        '2*1_pyramid': 8,\n",
        "                        '2*2': 9,\n",
        "                        '6*1': 10\n",
        "                    }\n",
        "        self.seq_train = iaa.Sequential([\n",
        "            iaa.CropAndPad(percent=(0, 0.1)),\n",
        "            iaa.Affine(rotate=(-20, 20)),\n",
        "            iaa.Resize({\"height\": Config.IMAGE_RESIZE, \"width\": Config.IMAGE_RESIZE}),\n",
        "            iaa.MultiplyAndAddToBrightness(mul=(0.5, 1.5), add=(-30, 30)),\n",
        "            iaa.GammaContrast((0.5, 2.0), per_channel=True),\n",
        "            iaa.GaussianBlur((0, 3.0)),\n",
        "            iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
        "            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
        "            iaa.ContrastNormalization((0.75, 1.5)),\n",
        "            iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
        "            iaa.AddToHueAndSaturation((-20, 20)),\n",
        "            iaa.Affine(shear=(-15, 15)),\n",
        "            iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}),\n",
        "            iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}),\n",
        "            iaa.Flipud(0.3),\n",
        "            iaa.Fliplr(0.3),\n",
        "            iaa.Dropout((0.01, 0.1), per_channel=0.5)\n",
        "        ])\n",
        "\n",
        "        self.seq_test = iaa.Sequential([\n",
        "            iaa.Resize({\"height\": Config.IMAGE_RESIZE, \"width\": Config.IMAGE_RESIZE})\n",
        "        ])\n",
        "\n",
        "        self.augmentation_factor = augmentation_factor\n",
        "        self.do_aug = do_aug\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) * self.augmentation_factor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Determine the original index in the annotations\n",
        "        original_index = idx // self.augmentation_factor\n",
        "        sample = self.data.iloc[original_index]\n",
        "\n",
        "        img_name = sample['filename']\n",
        "        img_path = os.path.join(self.root_dir,'images', img_name + '.png')\n",
        "        image = np.array(Image.open(img_path))[..., :3]\n",
        "\n",
        "        # Extract target variables\n",
        "        brick_type = self.brick_type_dict[sample['brick_type']]\n",
        "\n",
        "       \n",
        "        # Apply augmentations\n",
        "        if self.do_aug:\n",
        "            image = self.seq_train(image=image)\n",
        "        else :\n",
        "            image = self.seq_test(image=image)\n",
        "        \n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return (\n",
        "            image,\n",
        "            torch.tensor(brick_type).long()\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Early stop system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, delta=0, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet 34 Finetune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class LegoModel(nn.Module):\n",
        "    def __init__(self, num_brick_types = 11, inference = True):\n",
        "        super(LegoModel, self).__init__()\n",
        "\n",
        "        self.inference = inference\n",
        "\n",
        "         # Load resnet34 pre-trained on ImageNet\n",
        "        model = models.resnet34(pretrained=True)\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            model.conv1, \n",
        "            model.bn1, \n",
        "            model.relu, \n",
        "            model.maxpool, \n",
        "            model.layer1, \n",
        "            model.layer2, \n",
        "            model.layer3, \n",
        "            model.layer4\n",
        "        )\n",
        "        \n",
        "        # Modify the final layer to handle multiple outputs\n",
        "        num_features = model.fc.in_features\n",
        "        \n",
        "        # Freeze backbone\n",
        "        # optional for speed or few-shot-learning\n",
        "        if Config.FREEZE_BACKBONE :\n",
        "            for param in self.backbone.parameters() :\n",
        "                param.requires_grad = False\n",
        "        \n",
        "        # Define the new final layers for our multi-output prediction\n",
        "        self.fc_brick_type = nn.Linear(num_features, num_brick_types)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.mean(dim=(2, 3))\n",
        "\n",
        "        brick_type = self.fc_brick_type(x)\n",
        "\n",
        "        if self.inference:\n",
        "            brick_type = torch.softmax(brick_type, dim=1)\n",
        "\n",
        "        return brick_type\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train(model, data_loader, optimizer, device, epoch):\n",
        "\n",
        "    data_loader.dataset.dataset.do_aug = True\n",
        "    data_loader.dataset.dataset.augmentation_factor = Config.AUGMENTATION_FACTOR\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    pbar = tqdm(data_loader)\n",
        "\n",
        "    for batch_idx, (\n",
        "        image,\n",
        "        brick_type\n",
        "    ) in enumerate(pbar, 0):\n",
        "        \n",
        "        image = image.to(device)\n",
        "        brick_type = brick_type.to(device)\n",
        "        \n",
        "        # Optimizer zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Model inference\n",
        "        out_brick_type = model(image)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion_brick_type(out_brick_type, brick_type)\n",
        "        \n",
        "\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    avg_loss = train_loss / len(data_loader)\n",
        "\n",
        "    # return avg_loss, loss_brick_type, loss_rotation, loss_color\n",
        "    return avg_loss\n",
        "\n",
        "def test(model, data_loader, device, epoch):\n",
        "\n",
        "    data_loader.dataset.dataset.do_aug = False\n",
        "    data_loader.dataset.dataset.augmentation_factor = 1\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss_brick_type = 0.0\n",
        "    test_loss = 0.0\n",
        "    \n",
        "    # Initialize accumulators\n",
        "    total_type_correct = 0\n",
        "   \n",
        "\n",
        "    pbar = tqdm(data_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_idx, (\n",
        "            image,\n",
        "            brick_type\n",
        "        ) in enumerate(pbar, 0):\n",
        "            \n",
        "            image = image.to(device)\n",
        "            brick_type = brick_type.to(device)\n",
        "       \n",
        "            # Forward pass\n",
        "            out_brick_type = model(image)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss_brick = criterion_brick_type(out_brick_type, brick_type)\n",
        "            loss = loss_brick\n",
        "\n",
        "            pbar.set_description(f\"TEST loss={float(loss)}\")\n",
        "\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "\n",
        "            \n",
        "            # Gender accuracy\n",
        "            type_correct = (torch.argmax(out_brick_type, dim=1) == brick_type).float().sum()\n",
        "            total_type_correct += type_correct\n",
        "\n",
        "     \n",
        "    avg_loss = test_loss / len(data_loader)\n",
        "\n",
        "    # return avg_loss, loss_brick, loss_rotation, loss_color, type_correct\n",
        "    return avg_loss, type_correct\n",
        "\n",
        "criterion_brick_type = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8OBB4Ahxdu5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main(experiment):\n",
        "\n",
        "\n",
        "    # define Device\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "    # Create project folder and names\n",
        "    OUTPUT_FOLDER = os.path.join(Config.OUTPUT, experiment, datetime.datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\"))\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    print('OUTPUT_FOLDER',OUTPUT_FOLDER)\n",
        "\n",
        "\n",
        "\n",
        "    os.makedirs(Config.SUMMARY_PATH, exist_ok=True)\n",
        "    os.makedirs(Config.DEBUG_PATH, exist_ok=True)\n",
        "\n",
        "    CHECKPOINTS_FOLDER = os.path.join(OUTPUT_FOLDER, 'checkpoints')\n",
        "    os.makedirs(CHECKPOINTS_FOLDER, exist_ok=True)\n",
        "\n",
        "    # Define the data transformations\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Define the dataset and data loader\n",
        "    lego_dataset = LegoDataset('data.csv', './data', transform=data_transform)\n",
        "    train_size = int(Config.TRAIN_SPLIT * len(lego_dataset))\n",
        "    test_size = len(lego_dataset) - train_size\n",
        "    print('train_size',train_size)\n",
        "    print('test_size',test_size)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(lego_dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialize the model and define the loss function and optimizer\n",
        "    model = LegoModel(inference=False)\n",
        "\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.LR, eps = Config.EPS)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Epochs\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    best_model = None\n",
        "    best_type_accuracy = None\n",
        "\n",
        "    # Initialize accumulators\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    epochs_type_accuracy  = []\n",
        "\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(Config.EPOCHS):\n",
        "        \n",
        "        \n",
        "        train_loss = train(model, train_loader, optimizer, device, epoch)\n",
        "        val_loss, total_type_correct = test(model, test_loader, device, epoch)\n",
        "\n",
        "        # Append losses for plotting\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Compute average metrics for the epoch\n",
        "        epoch_type_accuracy = total_type_correct / len(lego_dataset) * 100\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            best_model = model.state_dict()\n",
        "            best_gender_accuracy = epoch_type_accuracy\n",
        "        \n",
        "        # Checkpointing\n",
        "        if epoch > 0 and epoch % Config.CKPT_SAVE_INTERVAL == 0 :\n",
        "            print(f\"Saved checkpoint {epoch}\\n\")\n",
        "            model_path_solve = os.path.join(CHECKPOINTS_FOLDER, f\"ckpt_{epoch}_{val_loss}.pth\")\n",
        "            optimizer_path_solve = os.path.join(CHECKPOINTS_FOLDER, model_path_solve.replace('ckpt_', 'optim_'))\n",
        "            torch.save(model.state_dict(), model_path_solve)\n",
        "            torch.save(optimizer.state_dict(), optimizer_path_solve)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch + 1}/{Config.EPOCHS}\\n\"\n",
        "          f\"Type accuracy: {epoch_type_accuracy:.2f}%\\n\"\n",
        "          f\"Validation loss: {val_loss:.4f}\")\n",
        "        \n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Check for early stopping\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "       \n",
        "\n",
        "    # Save the best model\n",
        "    torch.save(best_model, os.path.join(CHECKPOINTS_FOLDER, f\"best_{best_epoch}_{best_val_loss}.pth\"))\n",
        "        \n",
        "\n",
        "\n",
        "    # Plot the learning curves\n",
        "    epochs = range(1, Config.EPOCHS + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(epochs, train_losses, label=\"Training\")\n",
        "    plt.plot(epochs, val_losses, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Type Output Learning Curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QwG_zQqk35Y-"
      },
      "outputs": [],
      "source": [
        "main('model_lego_sorter_type1')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
